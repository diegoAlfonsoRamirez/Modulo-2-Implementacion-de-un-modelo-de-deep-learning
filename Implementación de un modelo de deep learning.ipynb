{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/Users/ramir/Desktop/Carpeta/Nueva_carpeta/Programas/Laboratorios_IA/Lab_1/complaints.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['complaint_what_happened', 'product','consumer_disputed']].dropna()\n",
    "df = df[df['consumer_disputed'] != 'N/A']\n",
    "df = df[df['complaint_what_happened'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['product'] == 'Mortgage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>product</th>\n",
       "      <th>consumer_disputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121126</th>\n",
       "      <td>We purchased a new home from XXXX XXXX in XXXX...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124822</th>\n",
       "      <td>Nationstar Mortgage requested information that...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124867</th>\n",
       "      <td>I submitted a form requesting acknowledgement ...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124900</th>\n",
       "      <td>Nationstar Mortgage became a servicer of my mo...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124935</th>\n",
       "      <td>On XXXX XXXX, XXXX, I got a letter from Nation...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979232</th>\n",
       "      <td>PNC Bank refuses to put the account on our cre...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979770</th>\n",
       "      <td>We were in a current modification with SPS ( s...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979790</th>\n",
       "      <td>Urgent Please HelpI purchased my personal resi...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979925</th>\n",
       "      <td>I had a short sale back in 2009 which was nego...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982815</th>\n",
       "      <td>i am behind on my mortgage and was dealing wit...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8317 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   complaint_what_happened   product  \\\n",
       "121126   We purchased a new home from XXXX XXXX in XXXX...  Mortgage   \n",
       "124822   Nationstar Mortgage requested information that...  Mortgage   \n",
       "124867   I submitted a form requesting acknowledgement ...  Mortgage   \n",
       "124900   Nationstar Mortgage became a servicer of my mo...  Mortgage   \n",
       "124935   On XXXX XXXX, XXXX, I got a letter from Nation...  Mortgage   \n",
       "...                                                    ...       ...   \n",
       "3979232  PNC Bank refuses to put the account on our cre...  Mortgage   \n",
       "3979770  We were in a current modification with SPS ( s...  Mortgage   \n",
       "3979790  Urgent Please HelpI purchased my personal resi...  Mortgage   \n",
       "3979925  I had a short sale back in 2009 which was nego...  Mortgage   \n",
       "3982815  i am behind on my mortgage and was dealing wit...  Mortgage   \n",
       "\n",
       "        consumer_disputed  \n",
       "121126                Yes  \n",
       "124822                Yes  \n",
       "124867                Yes  \n",
       "124900                Yes  \n",
       "124935                Yes  \n",
       "...                   ...  \n",
       "3979232               Yes  \n",
       "3979770               Yes  \n",
       "3979790               Yes  \n",
       "3979925               Yes  \n",
       "3982815               Yes  \n",
       "\n",
       "[8317 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['consumer_disputed'] == 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramir\\AppData\\Local\\Temp\\ipykernel_50464\\2057698642.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_temporal = df_temporal.append(df[df['consumer_disputed'] == 'No'].iloc[:8317], ignore_index=True)\n",
      "C:\\Users\\ramir\\AppData\\Local\\Temp\\ipykernel_50464\\2057698642.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_temporal.append(df[df['consumer_disputed'] == 'Yes'], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>product</th>\n",
       "      <th>consumer_disputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denied sufficient time to complete a ( short s...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nationstar Mortgage withheld money from my mon...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My client is XXXX XXXX and I have been working...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My original note was owned by XXXX Bank and I ...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My mortgage was bought by Nationstar Mortgage ...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16629</th>\n",
       "      <td>PNC Bank refuses to put the account on our cre...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>We were in a current modification with SPS ( s...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>Urgent Please HelpI purchased my personal resi...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>I had a short sale back in 2009 which was nego...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>i am behind on my mortgage and was dealing wit...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16634 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 complaint_what_happened   product  \\\n",
       "0      Denied sufficient time to complete a ( short s...  Mortgage   \n",
       "1      Nationstar Mortgage withheld money from my mon...  Mortgage   \n",
       "2      My client is XXXX XXXX and I have been working...  Mortgage   \n",
       "3      My original note was owned by XXXX Bank and I ...  Mortgage   \n",
       "4      My mortgage was bought by Nationstar Mortgage ...  Mortgage   \n",
       "...                                                  ...       ...   \n",
       "16629  PNC Bank refuses to put the account on our cre...  Mortgage   \n",
       "16630  We were in a current modification with SPS ( s...  Mortgage   \n",
       "16631  Urgent Please HelpI purchased my personal resi...  Mortgage   \n",
       "16632  I had a short sale back in 2009 which was nego...  Mortgage   \n",
       "16633  i am behind on my mortgage and was dealing wit...  Mortgage   \n",
       "\n",
       "      consumer_disputed  \n",
       "0                    No  \n",
       "1                    No  \n",
       "2                    No  \n",
       "3                    No  \n",
       "4                    No  \n",
       "...                 ...  \n",
       "16629               Yes  \n",
       "16630               Yes  \n",
       "16631               Yes  \n",
       "16632               Yes  \n",
       "16633               Yes  \n",
       "\n",
       "[16634 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temporal = pd.DataFrame()\n",
    "df_temporal = df_temporal.append(df[df['consumer_disputed'] == 'No'].iloc[:8317], ignore_index=True)\n",
    "df = df_temporal.append(df[df['consumer_disputed'] == 'Yes'], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Join the cleaned tokens back into a sentence\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'complain_what_happened' column\n",
    "df['complaint_what_happened_cleaned'] = df['complaint_what_happened'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split sentences into batches of 50 words\n",
    "def split_sentences(row):\n",
    "    words = word_tokenize(row['complaint_what_happened_cleaned'])\n",
    "    batch_size = 50\n",
    "    batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]\n",
    "    # Pad the last batch with empty strings if needed\n",
    "    last_batch_size = len(batches[-1])\n",
    "    if last_batch_size < batch_size:\n",
    "        batches[-1] += [''] * (batch_size - last_batch_size)\n",
    "    return pd.Series({'complaint_what_happened': batches, 'consumer_disputed': row['consumer_disputed']})\n",
    "\n",
    "# Apply the function to each row\n",
    "new_df = df.apply(split_sentences, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the inner arrays into separate rows\n",
    "df_expanded = new_df.explode('complaint_what_happened')\n",
    "\n",
    "# Determine the maximum number of elements in any inner array\n",
    "max_elements = df_expanded['complaint_what_happened'].apply(len).max()\n",
    "\n",
    "# Create new columns from the exploded lists\n",
    "for i in range(max_elements):\n",
    "    df_expanded[f'Word{i + 1}'] = df_expanded['complaint_what_happened'].apply(lambda x: x[i] if len(x) > i else None)\n",
    "\n",
    "# Drop the original column\n",
    "df_expanded = df_expanded.drop('complaint_what_happened', axis=1)\n",
    "\n",
    "# Reset the index\n",
    "df_expanded = df_expanded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to merge\n",
    "columns_to_merge = df_expanded.columns[1:51]\n",
    "\n",
    "# Create a new column with the merged values\n",
    "df_expanded['complaint_what_happened'] = df_expanded[columns_to_merge].apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Drop the original columns\n",
    "df_expanded.drop(columns=columns_to_merge, inplace=True)\n",
    "\n",
    "# Keeping only the first column and the merged column\n",
    "df_expanded = df_expanded[['consumer_disputed', 'complaint_what_happened']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "texts = df_expanded['complaint_what_happened'].values\n",
    "labels = df_expanded['consumer_disputed'].map({'Yes': 1, 'No': 0}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into trainning and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "max_words = 23300\n",
    "max_len = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the Tokenizer to a file\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Loading the pre-trained Word2Vec model (Google's Word2Vec model)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('Google_word2vec.bin', binary=True)\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, word2vec_model.vector_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words and word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the model to use\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, word2vec_model.vector_size, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(GRU(16))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=0.00005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "707/707 [==============================] - 29s 38ms/step - loss: 0.6935 - accuracy: 0.5121 - val_loss: 0.6913 - val_accuracy: 0.5273\n",
      "Epoch 2/300\n",
      "707/707 [==============================] - 26s 37ms/step - loss: 0.6904 - accuracy: 0.5320 - val_loss: 0.6899 - val_accuracy: 0.5325\n",
      "Epoch 3/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6888 - accuracy: 0.5390 - val_loss: 0.6890 - val_accuracy: 0.5343\n",
      "Epoch 4/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6876 - accuracy: 0.5441 - val_loss: 0.6885 - val_accuracy: 0.5381\n",
      "Epoch 5/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6865 - accuracy: 0.5467 - val_loss: 0.6876 - val_accuracy: 0.5380\n",
      "Epoch 6/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6854 - accuracy: 0.5491 - val_loss: 0.6870 - val_accuracy: 0.5434\n",
      "Epoch 7/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6843 - accuracy: 0.5516 - val_loss: 0.6860 - val_accuracy: 0.5447\n",
      "Epoch 8/300\n",
      "707/707 [==============================] - 27s 38ms/step - loss: 0.6832 - accuracy: 0.5536 - val_loss: 0.6854 - val_accuracy: 0.5519\n",
      "Epoch 9/300\n",
      "707/707 [==============================] - 30s 42ms/step - loss: 0.6820 - accuracy: 0.5571 - val_loss: 0.6844 - val_accuracy: 0.5523\n",
      "Epoch 10/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6807 - accuracy: 0.5594 - val_loss: 0.6833 - val_accuracy: 0.5509\n",
      "Epoch 11/300\n",
      "707/707 [==============================] - 31s 43ms/step - loss: 0.6793 - accuracy: 0.5633 - val_loss: 0.6823 - val_accuracy: 0.5514\n",
      "Epoch 12/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6778 - accuracy: 0.5658 - val_loss: 0.6807 - val_accuracy: 0.5543\n",
      "Epoch 13/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6761 - accuracy: 0.5713 - val_loss: 0.6792 - val_accuracy: 0.5583\n",
      "Epoch 14/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6743 - accuracy: 0.5749 - val_loss: 0.6780 - val_accuracy: 0.5613\n",
      "Epoch 15/300\n",
      "707/707 [==============================] - 31s 43ms/step - loss: 0.6728 - accuracy: 0.5763 - val_loss: 0.6775 - val_accuracy: 0.5646\n",
      "Epoch 16/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6716 - accuracy: 0.5784 - val_loss: 0.6781 - val_accuracy: 0.5636\n",
      "Epoch 17/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6708 - accuracy: 0.5800 - val_loss: 0.6757 - val_accuracy: 0.5679\n",
      "Epoch 18/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6700 - accuracy: 0.5818 - val_loss: 0.6752 - val_accuracy: 0.5698\n",
      "Epoch 19/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6695 - accuracy: 0.5819 - val_loss: 0.6750 - val_accuracy: 0.5695\n",
      "Epoch 20/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6688 - accuracy: 0.5844 - val_loss: 0.6751 - val_accuracy: 0.5685\n",
      "Epoch 21/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6682 - accuracy: 0.5850 - val_loss: 0.6765 - val_accuracy: 0.5680\n",
      "Epoch 22/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6678 - accuracy: 0.5847 - val_loss: 0.6744 - val_accuracy: 0.5684\n",
      "Epoch 23/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6673 - accuracy: 0.5852 - val_loss: 0.6746 - val_accuracy: 0.5690\n",
      "Epoch 24/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6670 - accuracy: 0.5863 - val_loss: 0.6741 - val_accuracy: 0.5683\n",
      "Epoch 25/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6666 - accuracy: 0.5879 - val_loss: 0.6745 - val_accuracy: 0.5696\n",
      "Epoch 26/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6660 - accuracy: 0.5891 - val_loss: 0.6746 - val_accuracy: 0.5698\n",
      "Epoch 27/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6657 - accuracy: 0.5890 - val_loss: 0.6764 - val_accuracy: 0.5691\n",
      "Epoch 28/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6653 - accuracy: 0.5885 - val_loss: 0.6741 - val_accuracy: 0.5723\n",
      "Epoch 29/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6648 - accuracy: 0.5890 - val_loss: 0.6760 - val_accuracy: 0.5706\n",
      "Epoch 30/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6646 - accuracy: 0.5902 - val_loss: 0.6737 - val_accuracy: 0.5714\n",
      "Epoch 31/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6642 - accuracy: 0.5908 - val_loss: 0.6738 - val_accuracy: 0.5725\n",
      "Epoch 32/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6637 - accuracy: 0.5908 - val_loss: 0.6740 - val_accuracy: 0.5745\n",
      "Epoch 33/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6634 - accuracy: 0.5924 - val_loss: 0.6736 - val_accuracy: 0.5720\n",
      "Epoch 34/300\n",
      "707/707 [==============================] - 31s 45ms/step - loss: 0.6630 - accuracy: 0.5907 - val_loss: 0.6734 - val_accuracy: 0.5707\n",
      "Epoch 35/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6626 - accuracy: 0.5934 - val_loss: 0.6738 - val_accuracy: 0.5728\n",
      "Epoch 36/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6622 - accuracy: 0.5926 - val_loss: 0.6736 - val_accuracy: 0.5748\n",
      "Epoch 37/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6619 - accuracy: 0.5934 - val_loss: 0.6742 - val_accuracy: 0.5762\n",
      "Epoch 38/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6616 - accuracy: 0.5952 - val_loss: 0.6744 - val_accuracy: 0.5749\n",
      "Epoch 39/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6613 - accuracy: 0.5947 - val_loss: 0.6735 - val_accuracy: 0.5751\n",
      "Epoch 40/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6611 - accuracy: 0.5954 - val_loss: 0.6734 - val_accuracy: 0.5738\n",
      "Epoch 41/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6607 - accuracy: 0.5959 - val_loss: 0.6733 - val_accuracy: 0.5741\n",
      "Epoch 42/300\n",
      "707/707 [==============================] - 32s 45ms/step - loss: 0.6603 - accuracy: 0.5970 - val_loss: 0.6740 - val_accuracy: 0.5769\n",
      "Epoch 43/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6599 - accuracy: 0.5971 - val_loss: 0.6745 - val_accuracy: 0.5749\n",
      "Epoch 44/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6597 - accuracy: 0.5965 - val_loss: 0.6735 - val_accuracy: 0.5747\n",
      "Epoch 45/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6595 - accuracy: 0.5967 - val_loss: 0.6742 - val_accuracy: 0.5768\n",
      "Epoch 46/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6589 - accuracy: 0.5986 - val_loss: 0.6735 - val_accuracy: 0.5755\n",
      "Epoch 47/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6586 - accuracy: 0.5984 - val_loss: 0.6737 - val_accuracy: 0.5762\n",
      "Epoch 48/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6584 - accuracy: 0.5990 - val_loss: 0.6754 - val_accuracy: 0.5753\n",
      "Epoch 49/300\n",
      "707/707 [==============================] - 31s 43ms/step - loss: 0.6581 - accuracy: 0.6002 - val_loss: 0.6742 - val_accuracy: 0.5762\n",
      "Epoch 50/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6575 - accuracy: 0.6008 - val_loss: 0.6747 - val_accuracy: 0.5764\n",
      "Epoch 51/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6574 - accuracy: 0.5995 - val_loss: 0.6757 - val_accuracy: 0.5736\n",
      "Epoch 52/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6570 - accuracy: 0.6014 - val_loss: 0.6735 - val_accuracy: 0.5775\n",
      "Epoch 53/300\n",
      "707/707 [==============================] - 30s 43ms/step - loss: 0.6569 - accuracy: 0.6019 - val_loss: 0.6740 - val_accuracy: 0.5774\n",
      "Epoch 54/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6562 - accuracy: 0.6030 - val_loss: 0.6737 - val_accuracy: 0.5788\n",
      "Epoch 55/300\n",
      "707/707 [==============================] - 31s 45ms/step - loss: 0.6559 - accuracy: 0.6018 - val_loss: 0.6744 - val_accuracy: 0.5787\n",
      "Epoch 56/300\n",
      "707/707 [==============================] - 31s 44ms/step - loss: 0.6558 - accuracy: 0.6026 - val_loss: 0.6742 - val_accuracy: 0.5769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b5051dba90>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, batch_size=64, epochs=300, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 5s 14ms/step - loss: 0.6742 - accuracy: 0.5769\n",
      "Test loss: 0.6742064356803894\n",
      "Test accuracy: 0.5768890380859375\n"
     ]
    }
   ],
   "source": [
    "# Test the model accuracy\n",
    "score = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Test loss: {score[0]}')\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_json = model.to_json()\n",
    "# with open('model.json', 'w') as json_file:\n",
    "#    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
